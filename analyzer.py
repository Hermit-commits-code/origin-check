import math
import os
import re


class ForensicEngine:
    """v1.0.0: Stable Release forensic engine for code authorship analysis."""

    # Cognitive Load Weights: Higher they are, harder it is for humans to write quickly.
    FILE_WEIGHTS = {
        ".py": 1.30,
        ".rs": 1.60,
        ".c": 1.50,
        ".cpp": 1.50,  # Systems/Logic
        ".js": 1.15,
        ".ts": 1.20,
        ".java": 1.25,  # App Logic
        ".html": 0.40,
        ".css": 0.45,  # UI/Styles
        ".md": 0.10,
        ".txt": 0.05,
        ".json": 0.20,  # Documentation/Data
        "LICENSE": 0.02,
        ".gitignore": 0.02,  # Static Boilerplate
    }

    @classmethod
    def get_weight(cls, file_path):
        """Returns the effort-weight based on file extension."""
        _, ext = os.path.splitext(file_path)
        # Default to 0.8 for unknown files
        return cls.FILE_WEIGHTS.get(ext, 0.80)

    @staticmethod
    def calculate_complexity(code_text):
        """
        Decision Density Index:
        Counts logical branches (if, for, while, try, except, etc.).
        """
        if not code_text:
            return 0.0

        logic_patterns = [
            r"\bif\b",
            r"\belse\b",
            r"\bfor\b",
            r"\bwhile\b",
            r"\btry\b",
            r"\bexcept\b",
            r"\bwith\b",
            r"\bclass\b",
            r"\bdef\b",
            r"\bmatch\b",
            r"\bcase\b",
            r"\basync\b",
        ]

        total_logic = sum(len(re.findall(p, code_text)) for p in logic_patterns)
        lines = len(code_text.splitlines())

        # Returns logic density per line
        return (total_logic / lines) if lines > 0 else 0.0

    @staticmethod
    def calculate_entropy(code_text):
        """
        Shannon Entropy:
        Measures the randomness of character distribution.
        AI code is often statistically 'too smooth' compared to human code.
        """
        if not code_text:
            return 0.0

        # Frequency of each character
        freq = {char: code_text.count(char) for char in set(code_text)}
        length = len(code_text)

        # Entropy formula
        entropy = -sum(
            (count / length) * math.log2(count / length) for count in freq.values()
        )

        # Normalized to 0.0 - 1.0 (Typical code range is 3.5 to 5.0)
        return min(entropy / 8.0, 1.0)

    @classmethod
    def analyze_diff(cls, file_path, diff_text, delta_seconds):
        """
        The Master Formula:
        Calculates the probability that the code was generated by AI.
        """
        weight = cls.get_weight(file_path)
        complexity = cls.calculate_complexity(diff_text)
        entropy_signal = cls.calculate_entropy(diff_text)

        # Clean lines added count
        lines_added = len(
            [
                line
                for line in diff_text.splitlines()
                if line.startswith("+") and not line.startswith("+++")
            ]
        )

        if lines_added == 0:
            return 0.0
        if delta_seconds <= 0:
            delta_seconds = 1  # Avoid division by zero

        # The core forensic math
        # Higher complexity + Lower Time + Higher Weight = Higher Score
        # We use a tanh-like ceiling to cap the score at 100%

        # Formula: S = tanh((L * W * (1 + C)) / (T + 1))
        # Where L=Lines, W=Weight, C=Complexity, T=TimeDelta

        # Use log10 to prevent large time gaps from zeroing out the score
        time_factor = math.log10(delta_seconds + 1)

        # New weighted raw score
        raw_score = (lines_added * weight * (1 + complexity) * (2 - entropy_signal)) / (
            time_factor + 1
        )

        # Adjust normalization
        # Apply normalization to keep it as a percentage
        final_score = math.tanh(raw_score / 60)
        return round(final_score, 4)
